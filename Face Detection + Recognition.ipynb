{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import h5py\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from keras.models import load_model\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D,Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import callbacks\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "IMG_SHAPE = (image_size, image_size, 3)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                              include_top=False,\n",
    "                                              weights='imagenet')\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 2 #Change this as per the number of known persons we want to store in our database \n",
    "model = tf.keras.Sequential([\n",
    "                          base_model,\n",
    "                          keras.layers.GlobalAveragePooling2D(),\n",
    "                          keras.layers.Dense(classes, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import MobileNet\n",
    "\n",
    "# MobileNet was designed to work on 224 x 224 pixel input images sizes\n",
    "img_rows, img_cols = 224, 224 \n",
    "\n",
    "# Re-loads the MobileNet model without the top or FC layers\n",
    "MobileNet = MobileNet(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n",
    "\n",
    "# Here we freeze the last 4 layers \n",
    "# Layers are set to trainable as True by default\n",
    "for layer in MobileNet.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(MobileNet.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different face detector models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import numpy as np\n",
    "\n",
    "detector1 = MTCNN()\n",
    "detector2 = dlib.get_frontal_face_detector()\n",
    "modelFile = \"D:/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"D:/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "classifier2 = cv2.CascadeClassifier('D:/haarcascade_frontalface2.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        img = cv2.resize(img, None, fx=0.5, fy=0.5)\n",
    "        height, width = img.shape[:2]\n",
    "        img1 = img.copy()\n",
    "        img2 = img.copy()\n",
    "        img3 = img.copy()\n",
    "        # detect faces in the image\n",
    "        faces1 = detector1.detect_faces(img)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces2 = detector2(gray, 1)\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)),\n",
    "                                        1.0, (300, 300), (104.0, 117.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        faces3 = net.forward()\n",
    "        faces4 = classifier2.detectMultiScale(img)\n",
    "        \n",
    "        # display faces on the original image\n",
    "        for result in faces1:\n",
    "            x, y, w, h = result['box']\n",
    "            x1, y1 = x + w, y + h\n",
    "            cv2.rectangle(img, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "        cv2.putText(img, 'mtcnn', (30, 30), font, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        for result in faces2:\n",
    "            x = result.left()\n",
    "            y = result.top()\n",
    "            x1 = result.right()\n",
    "            y1 = result.bottom()\n",
    "            cv2.rectangle(img1, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "        cv2.putText(img1, 'dlib', (30, 30), font, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        for i in range(faces3.shape[2]):\n",
    "            confidence = faces3[0, 0, i, 2]\n",
    "            if confidence > 0.5:\n",
    "                box = faces3[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
    "                (x, y, x1, y1) = box.astype(\"int\")\n",
    "                cv2.rectangle(img2, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "        cv2.putText(img2, 'dnn', (30, 30), font, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "                \n",
    "        for result in faces4:\n",
    "            x, y, w, h = result\n",
    "            x1, y1 = x + w, y + h\n",
    "            cv2.rectangle(img3, (x, y), (x1, y1), (0, 0, 255), 2)\n",
    "        cv2.putText(img3, 'haar', (30, 30), font, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        h1 = cv2.hconcat([img3, img1])\n",
    "        h2 = cv2.hconcat([img, img2])\n",
    "        fin = cv2.vconcat([h1, h2])\n",
    "\n",
    "        cv2.imshow(\"mtcnn\", img)\n",
    "        cv2.imshow(\"dlib\", img1)\n",
    "        cv2.imshow(\"dnn\", img2)\n",
    "        cv2.imshow(\"haar\", img3)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "           \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('D:/haarcascade_frontalface2.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (300, 300))\n",
    "\n",
    "        # Save file in specified directory with unique name\n",
    "        file_name_path = r\"D:/dataset_for_face/Asha_\" + str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 1000: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Samples Taken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential, model_from_yaml, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPool2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import np_utils, plot_model\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import cv2\n",
    "\n",
    "\n",
    "np.random.seed(44) #MALATYA<3 \n",
    "img_h, img_w = 224, 224\n",
    "image_size = (128, 128)\n",
    "nbatch_size = 2\n",
    "nepochs = 100\n",
    "nb_classes = 2\n",
    "def load_data():\n",
    "    path = 'D:/dataset_for_face/'\n",
    "    files = os.listdir(path)\n",
    "    images = []\n",
    "    labels = []\n",
    "    for f in files:\n",
    "        img_array = cv2.imread(os.path.join(path,f))\n",
    "        new_array = cv2.resize(img_array, (img_h, img_w))\n",
    "        images.append(new_array)\n",
    "\n",
    "        if 'Asha' in f:\n",
    "            labels.append(0)\n",
    "        elif 'Chiku' in f:\n",
    "            labels.append(1)\n",
    "       \n",
    "    data = np.array(images)\n",
    "    data = np.array(data).reshape(-1, img_h, img_h, 3)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    labels = np_utils.to_categorical(labels, 2)\n",
    "    return data, labels\n",
    "print(\"load_data......\")\n",
    "x_train, y_train = load_data()\n",
    "x_train = x_train/255\n",
    "print(x_train.shape,y_train.shape)\n",
    "checkpoint = callbacks.ModelCheckpoint(\"abcd.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint]\n",
    "model.fit(x_train, y_train, batch_size=nbatch_size, callbacks=callbacks, epochs=nepochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"evaluate......\")\n",
    "score, accuracy = model.evaluate(x_train[:3], y_train[:3], batch_size=nbatch_size)\n",
    "print('score:', score, 'accuracy:', accuracy)\n",
    "\n",
    "# Saving the mofel and making the reaining process much easier and mre practial. Saving the weights separatelyï¼š from\n",
    "# keras.models import model_from_yaml, load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"abcd.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "def markAttendance(name):\n",
    "    with open('Attendance.csv', 'r+') as f:\n",
    "        myDataList = f.readlines()\n",
    "\n",
    "\n",
    "        nameList = []\n",
    "        for line in myDataList:\n",
    "            entry = line.split(',')\n",
    "            nameList.append(entry[0])\n",
    "            if name not in nameList:\n",
    "                now = datetime.now()\n",
    "                dtString = now.strftime('%H:%M:%S')\n",
    "                f.writelines(f'\\n{name},{dtString}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "from keras.models import Sequential, model_from_yaml, load_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "import cv2\n",
    "\n",
    "detector = cv2.CascadeClassifier('D:/haarcascade_frontalface2.xml')\n",
    "img_h, img_w = 224,224\n",
    "\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    faces = detector.detectMultiScale(frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        ROI = frame[y:y + h, x:x + w]\n",
    "        for f in faces:\n",
    "            new_array = cv2.resize(ROI, (img_h, img_w))\n",
    "            data = np.array(new_array)\n",
    "            data = np.array(data).reshape(-1, img_h, img_h, 3)\n",
    "            result = model.predict_classes(data, verbose=0)\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            if result[0] == 0:\n",
    "                cv2.putText(frame, 'Chiku', (x, y), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 0, 255), 2)\n",
    "            if result[0] == 1:\n",
    "                cv2.putText(frame, 'Asha', (x, y), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 0, 255), 2)\n",
    "            \n",
    "                \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asha.jpg']\n",
      "['Asha']\n",
      "Encoding Complete\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# from PIL import ImageGrab\n",
    "\n",
    "path = 'Training_images'\n",
    "images = []\n",
    "classNames = []\n",
    "myList = os.listdir(path)\n",
    "print(myList)\n",
    "for cl in myList:\n",
    "    curImg = cv2.imread(f'{path}/{cl}')\n",
    "    images.append(curImg)\n",
    "    classNames.append(os.path.splitext(cl)[0])\n",
    "print(classNames)\n",
    "\n",
    "\n",
    "def findEncodings(images):\n",
    "    encodeList = []\n",
    "\n",
    "\n",
    "    for img in images:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        encode = face_recognition.face_encodings(img)[0]\n",
    "        encodeList.append(encode)\n",
    "    return encodeList\n",
    "    \n",
    "\n",
    "def markAttendance(name):\n",
    "    with open('Attendance.csv', 'r+') as f:\n",
    "        myDataList = f.readlines()\n",
    "\n",
    "\n",
    "        nameList = []\n",
    "        for line in myDataList:\n",
    "            entry = line.split(',')\n",
    "            nameList.append(entry[0])\n",
    "            if name not in nameList:\n",
    "                now = datetime.now()\n",
    "                dtString = now.strftime('%H:%M:%S')\n",
    "                f.writelines(f'\\n{name},{dtString}')\n",
    "\n",
    "#### FOR CAPTURING SCREEN RATHER THAN WEBCAM\n",
    "# def captureScreen(bbox=(300,300,690+300,530+300)):\n",
    "#     capScr = np.array(ImageGrab.grab(bbox))\n",
    "#     capScr = cv2.cvtColor(capScr, cv2.COLOR_RGB2BGR)\n",
    "#     return capScr\n",
    "\n",
    "encodeListKnown = findEncodings(images)\n",
    "print('Encoding Complete')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "# img = captureScreen()\n",
    "    imgS = cv2.resize(img, (0, 0), None, 0.25, 0.25)\n",
    "    imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    facesCurFrame = face_recognition.face_locations(imgS)\n",
    "    encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)\n",
    "\n",
    "    for encodeFace, faceLoc in zip(encodesCurFrame, facesCurFrame):\n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "# print(faceDis)\n",
    "        matchIndex = np.argmin(faceDis)\n",
    "\n",
    "        if matches[matchIndex]:\n",
    "            name = classNames[matchIndex].upper()\n",
    "# print(name)\n",
    "            y1, x2, y2, x1 = faceLoc\n",
    "            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.rectangle(img, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n",
    "            cv2.putText(img, name, (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "            markAttendance(name)\n",
    "\n",
    "    cv2.imshow('Webcam', img)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if matches[matchIndex]:\n",
    "            name = classNames[matchIndex].upper()\n",
    "            y1, x2, y2, x1 = faceLoc\n",
    "            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.rectangle(img, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n",
    "            cv2.putText(img, name, (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "        if not matches[matchIndex]:\n",
    "            name = 'Un'\n",
    "            y1, x2, y2, x1 = faceLoc\n",
    "            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.rectangle(img, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n",
    "            cv2.putText(img, name, (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obama.jpg']\n",
      "['obama']\n",
      "All Encodings Complete!!!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Feb  6 21:25:27 2021\n",
    "\n",
    "@author: hp\n",
    "\"\"\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "fpsLimit = 1 # throttle limit\n",
    "\n",
    "path = 'Training_images1'\n",
    "#path = 'Training_images2'\n",
    "images = []\n",
    "personNames = []\n",
    "myList = os.listdir(path)\n",
    "print(myList)\n",
    "for cu_img in myList:\n",
    "    current_Img = cv2.imread(f'{path}/{cu_img}')\n",
    "    images.append(current_Img)\n",
    "    personNames.append(os.path.splitext(cu_img)[0])\n",
    "print(personNames)\n",
    "\n",
    "\n",
    "def faceEncodings(images):\n",
    "    encodeList = []\n",
    "    for img in images:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        encode = face_recognition.face_encodings(img)[0]\n",
    "        encodeList.append(encode)\n",
    "    return encodeList\n",
    "\n",
    "\n",
    "def attendance(name):\n",
    "    with open('Attendance.csv', 'r+') as f:\n",
    "        myDataList = f.readlines()\n",
    "        nameList = []\n",
    "        for line in myDataList:\n",
    "            entry = line.split(',')\n",
    "            nameList.append(entry[0])\n",
    "            if name not in nameList:\n",
    "                time_now = datetime.now()\n",
    "                tStr = time_now.strftime('%H:%M:%S')\n",
    "                f.writelines(f'\\n{name},{tStr}')\n",
    "\n",
    "\n",
    "encodeListKnown = faceEncodings(images)\n",
    "print('All Encodings Complete!!!')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "startTime = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    nowTime = time.time()\n",
    "    if (int(nowTime - startTime)) > fpsLimit:\n",
    "        faces = cv2.resize(frame, (0, 0), None, 0.25, 0.25)\n",
    "        faces = cv2.cvtColor(faces, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        facesCurrentFrame = face_recognition.face_locations(faces)\n",
    "        encodesCurrentFrame = face_recognition.face_encodings(faces, facesCurrentFrame)\n",
    "\n",
    "        for encodeFace, faceLoc in zip(encodesCurrentFrame, facesCurrentFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "            # print(faceDis)\n",
    "            matchIndex = np.argmin(faceDis)\n",
    "\n",
    "            if matches[matchIndex]:\n",
    "                name = personNames[matchIndex].upper()\n",
    "                # print(name)\n",
    "                y1, x2, y2, x1 = faceLoc\n",
    "                y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n",
    "                cv2.putText(frame, name, (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            if not matches[matchIndex]:\n",
    "                name = 'Unknown'\n",
    "                y1, x2, y2, x1 = faceLoc\n",
    "                y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n",
    "                cv2.putText(frame, name, (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "                t=time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "                file=t+'.jpg'\n",
    "                cv2.imwrite(file,frame)\n",
    "                attendance(name)\n",
    "            startTime = time.time()\n",
    "\n",
    "            cv2.imshow('Webcam', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "fpsLimit = 1 # throttle limit\n",
    "startTime = time.time()\n",
    "cv = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cv.read()\n",
    "    nowTime = time.time()\n",
    "    if (int(nowTime - startTime)) > fpsLimit:\n",
    "        t=time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        file=t+'.jpg'\n",
    "        cv2.imwrite(file,frame)\n",
    "        startTime = time.time() # reset time\n",
    "        \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
